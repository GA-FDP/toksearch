{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "TokSearch supports multiple methods of processing data after a ```Pipeline``` has been defined. ```compute_serial``` processes each shot in the ```Pipeline``` sequentially. This is fine for small problems. But, if either the number of shots or the amount of data per shot increases, then TokSearch supports multiple methods of parallelizing the computations. At present, three methods support parallel execution of ```Pipeline``` objects:\n",
    "\n",
    "- ```compute_multiprocessing```: This method uses [JobLib](https://joblib.readthedocs.io/en/stable/) to execute the ```Pipeline``` using node-local multiprocessing.\n",
    "- ```compute_ray```: Here, the [Ray](https://ray.readthedocs.io/en/latest/) framework is used to execute either node-local parallelization or fully distributed across multiple worker nodes.\n",
    "- ```compute_spark```: Like ```compute_ray```, here [Apache Spark](https://spark.apache.org/) allows both node-local and fully distributed computation.\n",
    "\n",
    "Spark and Ray offer similar capabilities and performance when used with TokSearch. In particular, both can be used in fully distributed fashion and parallelize the ```Pipeline``` processing across arbitrarily many compute nodes as long as a cluster is set up properly. Additionally, they each can be used to parallelize within a single node, taking advantage of multi-core systems. For small jobs, though, ```compute_multiprocessing``` method is often the fastest and lightest weight.\n",
    "\n",
    "## Partitioning\n",
    "\n",
    "```Pipeline``` parallelization, regardless of backend, works by partioning the list of shots into roughly equal length chunks and then assinging each chunk to a worker process. Once all chunks have been processed the resulting records are returned to the calling program and reassembled into a flattened list of ```Records```.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The following examples show usage of the ```compute*``` methods.\n",
    "\n",
    "A few prelimimaries first:\n",
    "\n",
    "We'll import some things that we'll need. Note that we're specifying that the environment variables ```MKL_NUM_THREADS```, ```NUMEXPR_NUM_THREADS```, and ```OMP_NUM_THREADS``` should all equal \"1\". This prevents numpy from using multithreading, which tends to have a negative affect on execution time when running under Spark and Ray. Normally we'd want to set these using a modulefile or in a default environment, but we'll specify them here just to be sure. Note that these variables must be set before numpy is imported.\n",
    "\n",
    "We'll also create a utility class for timing execution later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Context manager that we can use to time execution\n",
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        elapsed = time.time() - self.start\n",
    "        print('Ran in {0:.2f} s'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our pipeline. The particulars of the pipeline aren't critical. It just retrieves a few signals and performs a simple calculation, then returns just the result of the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from toksearch import Pipeline, MdsSignal\n",
    "    \n",
    "def create_pipeline(shots):\n",
    "    ipmhd_signal = MdsSignal(r'\\ipmhd', 'efit01')\n",
    "    ip_signal = MdsSignal(r'\\ipmeas', 'efit01')\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(shots)\n",
    "    pipeline.fetch('ipmhd', ipmhd_signal)\n",
    "    pipeline.fetch('ip', ip_signal)\n",
    "    @pipeline.map\n",
    "    def calc_max_ipmhd(rec):\n",
    "        rec['max_ipmhd'] = np.max(np.abs(rec['ipmhd']['data']))\n",
    "        \n",
    "    pipeline.keep(['max_ipmhd'])\n",
    "    return pipeline   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify a non-trivial number of shots and create a list of shot numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shots = 2000\n",
    "shots = list(range(165920, 165920+num_shots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "RUNNING WITH compute_serial\n",
      "Ran in 38.58 s\n"
     ]
    }
   ],
   "source": [
    "print('*'*80)\n",
    "print('RUNNING WITH compute_serial')\n",
    "pipeline = create_pipeline(shots)\n",
    "with Timer():\n",
    "    serial_result = pipeline.compute_serial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "RUNNING WITH compute_multiprocessing\n",
      "Ran in 5.80 s\n"
     ]
    }
   ],
   "source": [
    "print('*'*80)\n",
    "print('RUNNING WITH compute_multiprocessing')\n",
    "pipeline = create_pipeline(shots)\n",
    "with Timer():\n",
    "    multiproc_result = pipeline.compute_multiprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ray with _temp_dir = /mnt/beegfs/users/sammuli/tmp/tmpj4075hyz\n",
      "********************************************************************************\n",
      "RUNNING WITH compute_ray\n",
      "Ran in 9.89 s\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "# Create a dummy pipeline to initialize ray so we can benchmark without the startup overhead\n",
    "dummy_res = Pipeline([1,2]).compute_ray()\n",
    "list(dummy_res)\n",
    "\n",
    "print('*'*80)\n",
    "print('RUNNING WITH compute_ray')\n",
    "pipeline = create_pipeline(shots)\n",
    "with Timer():\n",
    "    ray_result = pipeline.compute_ray(numparts=8)\n",
    "    list(ray_result)\n",
    "    ray_result.cleanup() # This shuts down the ray cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "RUNNING WITH compute_spark\n",
      "MASTER: None\n",
      "[('spark.master', 'None'), ('spark.app.submitTime', '1712249973365'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'pyspark-shell')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran in 9.04 s\n"
     ]
    }
   ],
   "source": [
    "print('*'*80)\n",
    "print('RUNNING WITH compute_spark')\n",
    "\n",
    "pipeline = create_pipeline(shots)\n",
    "\n",
    "# Spark results are generated lazily, so calling compute_spark\n",
    "# just initializes spark. We won't count that in our timing.\n",
    "spark_result = pipeline.compute_spark(numparts=8)\n",
    "\n",
    "with Timer():\n",
    "    list(spark_result) # Converting to a list materializes the result\n",
    "spark_result.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summarizing the results:\n",
    "\n",
    "| **Backend**     | **Execution Time (s)** | **Speedup** |\n",
    "|-----------------|--------------------|-------------|\n",
    "| Serial          | 38.58              | -           |\n",
    "| Multiprocessing | 5.80               | 6.65x       |\n",
    "| Ray             | 9.89               | 3.90x       |\n",
    "| Spark           | 9.04               | 4.27x       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results were obtained using 8 cores and 8 partitions (specified in the ```numparts``` keyword argument in the ```compute_spark``` and ```compute_ray``` methods) on a saga compute node.\n",
    "\n",
    "This demonstrates that the multiprocessing backed will often beat Ray or Spark due to the various parallelization overhead issues. But, for much bigger jobs, both Spark and Ray are capable of fully distributed computing that utilizes multiple nodes in a cluster.\n",
    "\n",
    "Running a TokSearch script that utilizes these capabilities is covered [here](/distributed_computing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
