{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Basics\n",
    "\n",
    "The following example illustrates a simple processing pipeline and demonstrates the key parts of the API. \n",
    "\n",
    "The basic workflow with TokSearch is to first define a set of ```Signal``` objects which represent, in this case, MDSplus . Here we grab the measured plasma current, ```ipmeas```, and the calculated plasma current, ```ipmhd```, from the efit01 MDSplus tree.\n",
    "\n",
    "Assuming you haven't already installed TokSearch, installation instruction as available [here](/installation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from toksearch import MdsSignal\n",
    "\n",
    "ip_signal = MdsSignal(r'\\ipmeas', 'efit01')\n",
    "ipmhd_signal = MdsSignal(r'\\ipmhd', 'efit01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we instantiate a ```Pipeline``` object with a list of the shots that we want to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from toksearch import Pipeline\n",
    "\n",
    "shots = [165920, 165921]\n",
    "pipeline = Pipeline(shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then pass the ```pipeline``` object the previously created signal objects and give them labels using the ```fetch``` method. A key point to understand here is that the ```fetch``` method does not immediately retrieve the requested data. It defers execution until explicitly requested (more on this later).\n",
    "\n",
    "Also note that when calling the ```fetch``` method the first argument is a label for the signal which will later become a field in a ```Record``` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fetch('ip', ip_signal)\n",
    "pipeline.fetch('ipmhd', ipmhd_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At this point we can inspect what the pipeline is doing by running one of the ```compute*``` family of methods. At the time of this writing, there are four supported ways of running the pipeline:\n",
    "\n",
    "- ```compute_serial```\n",
    "- ```compute_spark```\n",
    "- ```compute_ray```\n",
    "- ```compute_multiprocessing```\n",
    "\n",
    "```compute_serial```, as the name suggests, processes each shot serially. In our current example, it would process shot 165920, followed by 165921.\n",
    "\n",
    "The other methods will partition the list of shots into roughly equal sized chunks and process those chunks in parallel using the specified distributed computing framework (i.e. Apache Spark, Ray, or multiprocessing on a single node).\n",
    "\n",
    "For our example we'll use ```compute_serial```. All of these method return a list-like object that contains the resulting sequence of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 2. Should be 2.\n",
      "********************************************************************************\n",
      "shot: 165920\n",
      "errors:\n",
      "ip:\n",
      "\tdata: [210912.6 274596.4 280484.8 ... 476252.1 476977.2 470931.2]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "ipmhd:\n",
      "\tdata: [213493.6 281801.6 286739.6 ... 475302.8 474772.5 471811.7]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "********************************************************************************\n",
      "shot: 165921\n",
      "errors:\n",
      "ip:\n",
      "\tdata: [211574.8 274321.6 282818.1 ... 486420.2 486566.7 482745.8]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "ipmhd:\n",
      "\tdata: [213254.  282727.8 286100.  ... 485212.5 483553.4 481211.4]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    from collections import Mapping\n",
    "except ImportError:\n",
    "    from collections.abc import Mapping\n",
    "\n",
    "import collections\n",
    "import pprint\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=3, precision=1)\n",
    "\n",
    "records = pipeline.compute_serial()\n",
    "print('Number of records: {}. Should be 2.'.format(len(records)))\n",
    "\n",
    "# Helper function for printing results\n",
    "def pretty_print(record):\n",
    "    \n",
    "    for key in record.keys():\n",
    "        val = record[key]\n",
    "        if isinstance(val, Mapping):\n",
    "            print('{}:'.format(key))\n",
    "            for subkey, subval in val.items():\n",
    "                print('\\t{}: {}'.format(subkey, subval))\n",
    "        else:\n",
    "           print('{}: {}'.format(key, val))\n",
    "\n",
    "# Note the list-like behavior of the records result\n",
    "for record in records:\n",
    "    print('*'*80)\n",
    "    pretty_print(record)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note about the result:\n",
    "\n",
    "- Each ```Record``` object in ```results``` will always contain two attributes: ```shot``` and ```errors```. The ```errors``` attribute is a dictionary that stores information about any exceptions that occurred during execution of the pipeline for that shot. In this case, there were no exceptions, so the ```errors``` attribute is just an empty dict.\n",
    "\n",
    "- Recall that when we made the calls to ```pipeline.fetch(...)```, we specified the labels 'ip' and 'ipmhd'. Those now show up in the results records as fields. The default behavior for the ```fetch``` operation is to return a dictionary with the fields ```data``` and ```times```, each of which is a numpy array.\n",
    "\n",
    "At this point we haven't done anything terribly interesting. Let's now do some more processing by applying a ```map``` operation to each ```Record``` in the pipeline.\n",
    "\n",
    "We'll define a function, ```max_currents```, that calculates the maximum absolute value of both ```ip``` and ```ipmhd```. Functions passed to ```map``` take a single ```Record``` object as input, and then modify that object in place (returning nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.map\n",
    "def max_currents(record):\n",
    "    record['max_ip'] = np.max(np.abs(record['ip']['data']))\n",
    "    record['max_ipmhd'] = np.max(np.abs(record['ipmhd']['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using the decorator formulation of ```max_currents```. We could equivalently have done this:\n",
    "\n",
    "```python\n",
    "def max_currents(record):\n",
    "    record['max_ip'] = np.max(np.abs(record['ip']['data']))\n",
    "    record['max_ipmhd'] = np.max(np.abs(record['ipmhd']['data']))\n",
    "    \n",
    "pipeline.map(max_currents)\n",
    "```\n",
    "\n",
    "Let's run ```compute_serial``` again and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot: 165920\n",
      "errors:\n",
      "ip:\n",
      "\tdata: [210912.6 274596.4 280484.8 ... 476252.1 476977.2 470931.2]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "ipmhd:\n",
      "\tdata: [213493.6 281801.6 286739.6 ... 475302.8 474772.5 471811.7]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "max_ip: 1139184.875\n",
      "max_ipmhd: 1129914.0\n",
      "shot: 165921\n",
      "errors:\n",
      "ip:\n",
      "\tdata: [211574.8 274321.6 282818.1 ... 486420.2 486566.7 482745.8]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "ipmhd:\n",
      "\tdata: [213254.  282727.8 286100.  ... 485212.5 483553.4 481211.4]\n",
      "\ttimes: [ 100.  140.  160. ... 6340. 6360. 6380.]\n",
      "\tunits: {'data': 'A', 'times': 'ms'}\n",
      "max_ip: 1133248.875\n",
      "max_ipmhd: 1124894.625\n"
     ]
    }
   ],
   "source": [
    "records = pipeline.compute_serial()\n",
    "\n",
    "for record in records:\n",
    "    pretty_print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two records now have the fields ```max_ip``` and ```max_ipmhd``` as expected.\n",
    "\n",
    "For this simple example we are gathering all of the raw data used to calculate ```max_ip``` and ```max_ipmhd```. But, for cases with many more shots or many more pointnames, we can easily exceed the memory on the local machine. In those cases it's wise to only return the calculated quantities that we care about. We can use the ```Pipeline``` methods ```keep``` or ```discard``` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot: 165920\n",
      "errors:\n",
      "max_ip: 1139184.875\n",
      "max_ipmhd: 1129914.0\n",
      "shot: 165921\n",
      "errors:\n",
      "max_ip: 1133248.875\n",
      "max_ipmhd: 1124894.625\n"
     ]
    }
   ],
   "source": [
    "pipeline.keep(['max_ip', 'max_ipmhd'])\n",
    "\n",
    "records = pipeline.compute_serial()\n",
    "for record in records:\n",
    "    pretty_print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ```ip``` and ```ipmhd``` fields are no longer present in the records.\n",
    "\n",
    "Now let's suppose that we want to only find shots for which the maximum ```ip``` is above 1.15 MA. We implement this condition by creating a user-defined function that returns a boolean value. When run by the pipeline, if this function returns ```False``` the record will be removed from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(records): 1. Should be 1.\n",
      "shot: 165920\n",
      "errors:\n",
      "max_ip: 1139184.875\n",
      "max_ipmhd: 1129914.0\n"
     ]
    }
   ],
   "source": [
    "@pipeline.where\n",
    "def max_ip_is_high_enough(record):\n",
    "    return record['max_ip'] > 1.135e6\n",
    "\n",
    "records = pipeline.compute_serial()\n",
    "print('len(records): {}. Should be 1.'.format(len(records)))\n",
    "\n",
    "\n",
    "for record in records:\n",
    "    pretty_print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one of the two input shots (165920) matched the where criteria, so the length of ```records``` is 1."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
